{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-svm-w2v.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnZal9f6c3TyZSlugHyt7t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDdy1VLFP3eM",
        "colab_type": "text"
      },
      "source": [
        "# CNN + SVM para clasificar texto\n",
        "\n",
        "Este notebook está inspirado en el trabajo de este paper https://www.researchgate.net/publication/331701896_Short_Text_Classification_With_A_Convolutional_Neural_Networks_Based_Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZriIylSDYk1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f23b21dc-0911-4085-ff03-840feebec852"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as python_random\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "python_random.seed(123)\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Embedding, concatenate, Input\n",
        "from keras.layers import Conv1D, GlobalMaxPool1D, SpatialDropout1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot as plt\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.utils.fixes import loguniform\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def metrics(predictions, y_test):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
        "    print(f'Verdaderos Negativos: {tn}')\n",
        "    print(f'Falsos Negativos: {fn}')\n",
        "    print(f'Verdaderos Positivos: {tp}')\n",
        "    print(f'Falsos Positivos: {fp}')\n",
        "    print()\n",
        "    print(f'precision score: {precision_score(y_test, predictions)}')\n",
        "    print(f'recall score: {recall_score(y_test, predictions)}')\n",
        "    print(f'f1 score: {f1_score(y_test,  predictions)}')\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2_Fx3ThaA7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url_train = 'https://raw.githubusercontent.com/fsicardir/datos-tp2/master/dataset/train.csv?token=AFVAIUW66UE3NA5X2SYXNPC7GHGJY'\n",
        "url_test = 'https://raw.githubusercontent.com/fsicardir/datos-tp2/master/dataset/test.csv?token=AFVAIUUSBVEOOMDIFV4GU6C7GHGNK'\n",
        "\n",
        "read_train = pd.read_csv(url_train)\n",
        "read_test = pd.read_csv(url_test)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqWINsssaIX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "13fd1cdd-7f77-4863-8aa8-df798e1c8fc7"
      },
      "source": [
        "# Vamos a usar estos embeddings primero, y luego probar los de GloVe.\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-08 19:34:53--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.88.37\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.88.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyHqDjBKaoJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "b0deb6ec-9c32-4cd7-d775-f18cd56d6410"
      },
      "source": [
        "# Créditos a este notebook https://www.kaggle.com/nmaguette/up-to-date-list-of-slangs-for-text-preprocessing\n",
        "abbreviations = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"€\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\", \n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\", \n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"ic\" : \"i see\",\n",
        "    \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\", \n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "    \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\", \n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtf\" : \"what the fuck\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "}\n",
        "\n",
        "\n",
        "def convert_abbrev(word):\n",
        "    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
        "\n",
        "# Esta lista de contractions la obtuvimos de un notebook de Kaggle también, el cual pone como fuente al siguiente\n",
        "# post de stackoverflow http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\",\n",
        "\"thx\"   : \"thanks\"\n",
        "}\n",
        "\n",
        "\n",
        "def remove_contractions(text):\n",
        "    return contractions[text.lower()] if text.lower() in contractions.keys() else text\n",
        "\n",
        "df_train = read_train[['id', 'text', 'target']]\n",
        "df_test = read_test[['id', 'text']]\n",
        "\n",
        "# Limpiamos los datos de la forma usual\n",
        "# Quitamos las urls\n",
        "df_train['text'] = df_train['text'].str.replace(r'http:\\/\\/.*', '', regex=True).replace(r'https:\\/\\/.*', '', regex=True)\n",
        "df_test['text'] = df_test['text'].str.replace(r'http:\\/\\/.*', '', regex=True).replace(r'https:\\/\\/.*', '', regex=True)\n",
        "\n",
        "# Quitamos user mentions, signos de puntuación, hashtags y stopwords.\n",
        "def clean_text(text):\n",
        "    words = text.lower().split(' ')\n",
        "    words = [convert_abbrev(word) for word in words]\n",
        "    words = [remove_contractions(word) for word in words]\n",
        "    text = ' '.join([word for word in words if not word.startswith('@') and word not in stopwords.words('english')])\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(clean_text)\n",
        "df_test['text'] = df_test['text'].apply(clean_text)\n",
        "\n",
        "train_tweets = df_train['text'].tolist()\n",
        "train_target = df_train['target']\n",
        "test_tweets = df_test['text'].tolist()\n",
        "len(train_tweets), len(train_target), len(test_tweets)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:324: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:325: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:336: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:337: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 7613, 3263)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fewF4U7bsJ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c12fbab-2e5d-4c0c-bc25-33d4c403a7dc"
      },
      "source": [
        "oov_token = \"<UNK>\"\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(train_tweets)\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(train_tweets)\n",
        "sequence_test = tokenizer.texts_to_sequences(test_tweets)\n",
        "\n",
        "max_padding = 25\n",
        "padded_vecs_train = pad_sequences(sequences_train, maxlen=max_padding, padding='post')\n",
        "padded_vecs_test = pad_sequences(sequence_test, maxlen=max_padding, padding='post')\n",
        "\n",
        "# Ahora vamos a crear una matriz que tendrá los embeddings de Google\n",
        "# correspondientes a cada palabra de nuestro vocabulario.\n",
        "# Esto se lo pasaremos como pesos a la capa de Embedding del modelo a entrenar.\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
        "oov_words = 0\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  try:\n",
        "    embedding_vector = word2vec[word]\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "  except:\n",
        "    oov_words += 1\n",
        "    continue\n",
        "\n",
        "oov_words"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EXwRIC-6vpG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f2009ff-66c5-4f48-f1eb-1c855ec391c5"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(padded_vecs_train, train_target, test_size=0.2, random_state=31)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape "
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6090, 25), (1523, 25), (6090,), (1523,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoerCc0RdZIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "1fb5f23f-56b8-405d-865c-7b8c6c233fd3"
      },
      "source": [
        "N_FILTERS = 128\n",
        "DENSE_DROPOUT = 0.5\n",
        "\n",
        "input_text = Input(shape=(max_padding))\n",
        "emb_text = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_padding, trainable=False)(input_text)\n",
        "\n",
        "conv1 = Conv1D(N_FILTERS, 3, padding='same', activation='relu')(emb_text)\n",
        "conv1 = GlobalMaxPool1D()(conv1)\n",
        "\n",
        "conv2 = Conv1D(N_FILTERS, 4, padding='same', activation='relu')(emb_text)\n",
        "conv2 = GlobalMaxPool1D()(conv2)\n",
        "\n",
        "conv3 = Conv1D(N_FILTERS, 5, padding='same', activation='relu')(emb_text)\n",
        "conv3 = GlobalMaxPool1D()(conv3)\n",
        "\n",
        "conv_output = concatenate([conv1, conv2, conv3], axis=1, name='concat_pooling')\n",
        "\n",
        "conv_output = Dense(64, activation='relu',  name='output_for_svc')(conv_output)\n",
        "conv_output = Dropout(DENSE_DROPOUT)(conv_output)\n",
        "\n",
        "prediction = Dense(1, activation='sigmoid')(conv_output)\n",
        "\n",
        "model_conv = Model(input_text, prediction)\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model_conv.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', f1_m])\n",
        "model_conv.summary()"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_82\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           [(None, 25)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_24 (Embedding)        (None, 25, 300)      4609200     input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_72 (Conv1D)              (None, 25, 128)      115328      embedding_24[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_73 (Conv1D)              (None, 25, 128)      153728      embedding_24[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_74 (Conv1D)              (None, 25, 128)      192128      embedding_24[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_72 (Global (None, 128)          0           conv1d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_73 (Global (None, 128)          0           conv1d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_74 (Global (None, 128)          0           conv1d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concat_pooling (Concatenate)    (None, 384)          0           global_max_pooling1d_72[0][0]    \n",
            "                                                                 global_max_pooling1d_73[0][0]    \n",
            "                                                                 global_max_pooling1d_74[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "output_for_svc (Dense)          (None, 64)           24640       concat_pooling[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 64)           0           output_for_svc[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 1)            65          dropout_24[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 5,095,089\n",
            "Trainable params: 485,889\n",
            "Non-trainable params: 4,609,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17d5_z-0im93",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e5f30e53-34a1-4278-efc2-097579fe0afc"
      },
      "source": [
        "history = model_conv.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64, verbose=True)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "96/96 [==============================] - 8s 80ms/step - loss: 0.5174 - accuracy: 0.7552 - f1_m: 0.6841 - val_loss: 0.4396 - val_accuracy: 0.8142 - val_f1_m: 0.7681\n",
            "Epoch 2/5\n",
            "96/96 [==============================] - 7s 78ms/step - loss: 0.3834 - accuracy: 0.8386 - f1_m: 0.7969 - val_loss: 0.4192 - val_accuracy: 0.8234 - val_f1_m: 0.7856\n",
            "Epoch 3/5\n",
            "96/96 [==============================] - 7s 77ms/step - loss: 0.3051 - accuracy: 0.8760 - f1_m: 0.8448 - val_loss: 0.4431 - val_accuracy: 0.8109 - val_f1_m: 0.7809\n",
            "Epoch 4/5\n",
            "96/96 [==============================] - 7s 78ms/step - loss: 0.2083 - accuracy: 0.9250 - f1_m: 0.9078 - val_loss: 0.5005 - val_accuracy: 0.8135 - val_f1_m: 0.7805\n",
            "Epoch 5/5\n",
            "96/96 [==============================] - 7s 78ms/step - loss: 0.1572 - accuracy: 0.9479 - f1_m: 0.9369 - val_loss: 0.5507 - val_accuracy: 0.7774 - val_f1_m: 0.7576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVFH7ikpjg__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor = Model(model_conv.inputs, model_conv.get_layer('output_for_svc').output)\n",
        "features_train = extractor.predict(X_train)\n",
        "features_val = extractor.predict(X_test)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcRAE2smtvO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae50146a-cfd0-4dcf-c40d-9d1fd253b961"
      },
      "source": [
        "features_train.shape, features_val.shape"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6090, 64), (1523, 64))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js4oEgF03M8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ce46bdd0-2757-4413-8b5b-2b758ec019bd"
      },
      "source": [
        "svc = svm.SVC()\n",
        "\n",
        "params = {'C': loguniform(1e0, 1e3),\n",
        "          'gamma': loguniform(1e-4, 1e-3),\n",
        "          'kernel': ['rbf', 'linear', 'poly']\n",
        "}\n",
        "\n",
        "grid = RandomizedSearchCV(svc, param_distributions=params, verbose=True, cv=5, n_iter=10, n_jobs=-1)\n",
        "grid.fit(features_train, y_train)\n",
        "\n",
        "grid.best_params_, grid.best_score_"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   49.6s\n",
            "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   51.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'C': 2.724380743522747, 'gamma': 0.0004788032177166067, 'kernel': 'rbf'},\n",
              " 0.9674876847290641)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHWjjSWJ4Ofy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "1233e23e-4bf5-4d54-ea5a-57f22820fb40"
      },
      "source": [
        "preds = grid.predict(features_val)\n",
        "metrics(preds, y_test)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Verdaderos Negativos: 711\n",
            "Falsos Negativos: 159\n",
            "Verdaderos Positivos: 510\n",
            "Falsos Positivos: 143\n",
            "\n",
            "precision score: 0.781010719754977\n",
            "recall score: 0.7623318385650224\n",
            "f1 score: 0.7715582450832071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he26usRHOA_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_test = extractor.predict(padded_vecs_test)\n",
        "kaggle_preds = grid.predict(features_test)\n",
        "\n",
        "results = df_test[['id']]\n",
        "results['target'] = kaggle_preds\n",
        "results.to_csv('cnn-svm-v1.csv', index=False)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfHRIGK5UNen",
        "colab_type": "text"
      },
      "source": [
        "# Otra CNN al momento + SVM\n",
        "\n",
        "Ahora vamos a intentar correr el modelo que mejor nos dio pero con otros embeddings, y aplicar la misma idea de utilizar los pesos de la última capa fully connected para usar a modo de features al entrenar SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyJxedlpO4I4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Este modelo sacó 0.81274 en Kaggle.\n",
        "# Pasamos a la api funcional para poder meter filtros en paralelo.\n",
        "# No hay justificación para el dropout en la capa de Embedding, it just worked.\n",
        "n_filters = 200\n",
        "\n",
        "inputs = Input(shape=(max_padding))\n",
        "emb = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_padding, trainable=False)(inputs)\n",
        "drop = Dropout(0.5)(emb)\n",
        "\n",
        "conv1 = Conv1D(n_filters, 5, padding='same', activation='relu')(drop)\n",
        "conv1 = GlobalMaxPool1D()(conv1)\n",
        "conv1 = Dropout(0.5)(conv1)\n",
        "\n",
        "conv2 = Conv1D(n_filters, 4, padding='same', activation='relu')(drop)\n",
        "conv2 =  GlobalMaxPool1D()(conv2)\n",
        "conv2 = Dropout(0.5)(conv2)\n",
        "\n",
        "conv3 = Conv1D(n_filters, 3, padding='same', activation='relu')(drop)\n",
        "conv3 = GlobalMaxPool1D()(conv3)\n",
        "conv3 = Dropout(0.5)(conv3)\n",
        "\n",
        "concat = concatenate([conv1, conv2, conv3], axis=1)\n",
        "drop_concat = Dropout(0.25)(concat)\n",
        "out = Dense(128, activation='relu', name='output_for_svc')(drop_concat)\n",
        "out = Dropout(0.25)(out)\n",
        "out = Dense(1, activation='sigmoid')(out)\n",
        "\n",
        "model = Model(inputs, out)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5amW-b7S-dD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "4fed59c4-5075-4448-e695-5cfb61b8e211"
      },
      "source": [
        "epochs = 20\n",
        "model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test))"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "191/191 [==============================] - 13s 66ms/step - loss: 0.5748 - accuracy: 0.7025 - val_loss: 0.4680 - val_accuracy: 0.7905\n",
            "Epoch 2/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.4950 - accuracy: 0.7680 - val_loss: 0.4437 - val_accuracy: 0.8056\n",
            "Epoch 3/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.4736 - accuracy: 0.7856 - val_loss: 0.4359 - val_accuracy: 0.8122\n",
            "Epoch 4/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.4527 - accuracy: 0.7947 - val_loss: 0.4339 - val_accuracy: 0.8162\n",
            "Epoch 5/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.4386 - accuracy: 0.7966 - val_loss: 0.4291 - val_accuracy: 0.8240\n",
            "Epoch 6/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.4244 - accuracy: 0.8092 - val_loss: 0.4256 - val_accuracy: 0.8221\n",
            "Epoch 7/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.4038 - accuracy: 0.8200 - val_loss: 0.4237 - val_accuracy: 0.8207\n",
            "Epoch 8/20\n",
            "191/191 [==============================] - 12s 65ms/step - loss: 0.4011 - accuracy: 0.8263 - val_loss: 0.4227 - val_accuracy: 0.8194\n",
            "Epoch 9/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.3963 - accuracy: 0.8186 - val_loss: 0.4268 - val_accuracy: 0.8260\n",
            "Epoch 10/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.3771 - accuracy: 0.8348 - val_loss: 0.4236 - val_accuracy: 0.8234\n",
            "Epoch 11/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.3655 - accuracy: 0.8366 - val_loss: 0.4225 - val_accuracy: 0.8181\n",
            "Epoch 12/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.3556 - accuracy: 0.8502 - val_loss: 0.4314 - val_accuracy: 0.8181\n",
            "Epoch 13/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.3399 - accuracy: 0.8517 - val_loss: 0.4281 - val_accuracy: 0.8181\n",
            "Epoch 14/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.3348 - accuracy: 0.8534 - val_loss: 0.4334 - val_accuracy: 0.8201\n",
            "Epoch 15/20\n",
            "191/191 [==============================] - 13s 67ms/step - loss: 0.3177 - accuracy: 0.8573 - val_loss: 0.4408 - val_accuracy: 0.8214\n",
            "Epoch 16/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.3130 - accuracy: 0.8599 - val_loss: 0.4469 - val_accuracy: 0.8116\n",
            "Epoch 17/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.3039 - accuracy: 0.8686 - val_loss: 0.4536 - val_accuracy: 0.8116\n",
            "Epoch 18/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.3001 - accuracy: 0.8709 - val_loss: 0.4719 - val_accuracy: 0.8024\n",
            "Epoch 19/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.2914 - accuracy: 0.8782 - val_loss: 0.4709 - val_accuracy: 0.8129\n",
            "Epoch 20/20\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.2828 - accuracy: 0.8808 - val_loss: 0.4632 - val_accuracy: 0.8142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8fa9c8be10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MisMhqCHTPUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9da7f24-95ac-4fa1-f291-cdb69971e53e"
      },
      "source": [
        "extractor_v2 = Model(model.inputs, model.get_layer('output_for_svc').output)\n",
        "features_train_v2 = extractor_v2.predict(X_train)\n",
        "features_val_v2 = extractor_v2.predict(X_test)\n",
        "features_train_v2.shape, features_val_v2.shape"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6090, 128), (1523, 128))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u5NLZcCTr6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a11f59ba-9319-495d-862c-902ccad15e02"
      },
      "source": [
        "svc_v2 = svm.SVC()\n",
        "\n",
        "params = {'C': loguniform(1e0, 1e3),\n",
        "          'gamma': loguniform(1e-4, 1e-3)\n",
        "}\n",
        "\n",
        "grid_v2 = RandomizedSearchCV(svc_v2, param_distributions=params, verbose=True, cv=5, n_iter=20, n_jobs=-1)\n",
        "grid_v2.fit(features_train_v2, y_train)\n",
        "\n",
        "grid_v2.best_params_, grid_v2.best_score_"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   20.1s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   45.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'C': 208.3547464007065, 'gamma': 0.00020416800005588378}, 0.964367816091954)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5vXqZHGT5d-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "3eecfa7a-793b-47e1-e84c-b89e59240261"
      },
      "source": [
        "preds_v2 = grid_v2.predict(features_val_v2)\n",
        "metrics(preds_v2, y_test)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Verdaderos Negativos: 730\n",
            "Falsos Negativos: 174\n",
            "Verdaderos Positivos: 495\n",
            "Falsos Positivos: 124\n",
            "\n",
            "precision score: 0.7996768982229402\n",
            "recall score: 0.7399103139013453\n",
            "f1 score: 0.7686335403726708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7OLyr9iUFDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_test_v2 = extractor_v2.predict(padded_vecs_test)\n",
        "kaggle_preds_v2 = grid_v2.predict(features_test_v2)\n",
        "\n",
        "results = df_test[['id']]\n",
        "results['target'] = kaggle_preds_v2\n",
        "results.to_csv('cnn-svm-v2.csv', index=False)"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX47wqjHXcLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}