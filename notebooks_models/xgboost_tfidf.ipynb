{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ndv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', -1)\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../dataset/train.csv')\n",
    "df_test = pd.read_csv('../dataset/test.csv')\n",
    "df_train.drop(columns=['location', 'keyword'], inplace=True)\n",
    "df_test.drop(columns=['location', 'keyword'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quitamos las urls\n",
    "df_train['text'] = df_train['text'].str.replace(r'http:\\/\\/.*', '', regex=True).replace(r'https:\\/\\/.*', '', regex=True)\n",
    "df_test['text'] = df_test['text'].str.replace(r'http:\\/\\/.*', '', regex=True).replace(r'https:\\/\\/.*', '', regex=True)\n",
    "\n",
    "# Quitamos user mentions, signos de puntuaci칩n, hashtags y stopwords.\n",
    "def clean_text(text):\n",
    "    words = text.lower().split(' ')\n",
    "    text = ' '.join([word for word in words if not word.startswith('@') and word not in stopwords.words('english')])\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(clean_text)\n",
    "df_test['text'] = df_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = df_train['text']\n",
    "train_target = df_train['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_text, train_target, test_size=0.20, random_state=17)\n",
    "\n",
    "# Probaremos con unigramas primero por lo que dejaremos el ngram_range en su valor default.\n",
    "# S칩lo agregaremos al vocabulario aquellas palabras que aparecen m치s de 5 veces para filtrar palabras\n",
    "# poco frecuentes.\n",
    "tfidf = TfidfVectorizer(min_df=5)\n",
    "X_train_vecs = tfidf.fit_transform(X_train)\n",
    "X_test_vecs = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6090, 2169), (6090,), (1523, 2169), (1523,))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vecs.shape, y_train.shape, X_test_vecs.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=0.1, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constrain...\n",
       "                                     random_state=17, reg_alpha=None,\n",
       "                                     reg_lambda=None, scale_pos_weight=None,\n",
       "                                     subsample=None, tree_method=None,\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.7, 0.9],\n",
       "                         'max_depth': [3, 5, 10],\n",
       "                         'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora usaremos XGBoost para intentar predecir el target de los tweets utilizando\n",
    "# este nuevo formato como features.\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [x for x in range(10, 100, 10)],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=17, learning_rate=0.1)\n",
    "CV_xgb = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=5, scoring='f1', n_jobs=4)\n",
    "CV_xgb.fit(X_train_vecs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7, 'max_depth': 10, 'n_estimators': 90}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_classifier(classifier):\n",
    "    predictions = classifier.predict(X_test_vecs)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(f'Verdaderos Negativos: {tn}')\n",
    "    print(f'Falsos Negativos: {fn}')\n",
    "    print(f'Verdaderos Positivos: {tp}')\n",
    "    print(f'Falsos Positivos: {fp}')\n",
    "    print()\n",
    "    print(f'f1 score: {f1_score(y_test,  predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdaderos Negativos: 798\n",
      "Falsos Negativos: 320\n",
      "Verdaderos Positivos: 347\n",
      "Falsos Positivos: 58\n",
      "\n",
      "f1 score: 0.6473880597014926\n"
     ]
    }
   ],
   "source": [
    "try_classifier(CV_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora usaremos todos los datos del set de entrenamiento para entrenar el modelo que dio los\n",
    "# mejores resultados y luego haremos la predicci칩n para el set de test.\n",
    "best_xgb = CV_xgb.best_estimator_\n",
    "\n",
    "tfidf_pred = TfidfVectorizer(min_df=5)\n",
    "train_text_vecs = tfidf_pred.fit_transform(train_text)\n",
    "test_text_vecs = tfidf_pred.transform(df_test['text'])\n",
    "\n",
    "best_xgb.fit(train_text_vecs, train_target)\n",
    "kaggle_pred = best_xgb.predict(test_text_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>heard earthquake different cities stay safe everyone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>forest fire spot pond geese fleeing across street cannot save</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>typhoon soudelor kills 28 china taiwan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                           text  target\n",
       "0  0   happened terrible car crash                                    1     \n",
       "1  2   heard earthquake different cities stay safe everyone           1     \n",
       "2  3   forest fire spot pond geese fleeing across street cannot save  1     \n",
       "3  9   apocalypse lighting spokane wildfires                          0     \n",
       "4  11  typhoon soudelor kills 28 china taiwan                         1     "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['target'] = kaggle_pred\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['id', 'target']].to_csv('../submits/xgboost_tfidf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.7, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=90, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=17, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
